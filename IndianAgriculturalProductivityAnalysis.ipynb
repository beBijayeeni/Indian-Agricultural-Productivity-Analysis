{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdLwQQHfSMg909mtYlzGd2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beBijayeeni/Indian-Agricultural-Productivity-Analysis/blob/main/IndianAgriculturalProductivityAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SOURCE = \"https://www.kaggle.com/datasets/akshatgupta7/crop-yield-in-indian-states-dataset\"\n",
        "print(\"Data Source:\", DATA_SOURCE)"
      ],
      "metadata": {
        "id": "GJzW37kyaOxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About Dataset**\n",
        "\n",
        "This dataset encompasses agricultural data for multiple crops cultivated across various states in India from the year 1997 till 2020. The dataset provides crucial features related to crop yield prediction, including crop types, crop years, cropping seasons, states, areas under cultivation, production quantities, annual rainfall, fertilizer usage, pesticide usage, and calculated yields."
      ],
      "metadata": {
        "id": "59a4YZvFf8qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Columns Description:**\n",
        "\n",
        "1. **Crop**: The name of the crop cultivated.\n",
        "2. **Crop_Year**: The year in which the crop was grown.\n",
        "3. **Season**: The specific cropping season (e.g., Kharif, Rabi, Whole Year).\n",
        "4. **State**: The Indian state where the crop was cultivated.\n",
        "5. **Area**: The total land area (in hectares) under cultivation for the specific crop.\n",
        "6. **Production**: The quantity of crop production (in metric tons).\n",
        "7. **Annual_Rainfall**: The annual rainfall received in the crop-growing region (in mm).\n",
        "8. **Fertilizer**: The total amount of fertilizer used for the crop (in kilograms).\n",
        "9. **Pesticide**: The total amount of pesticide used for the crop (in kilograms).\n",
        "10. **Yield**: The calculated crop yield (production per unit area).\n"
      ],
      "metadata": {
        "id": "KJwQzpaHgVRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0U_qI8gMhB5X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVmSrS91a56Q"
      },
      "outputs": [],
      "source": [
        "#Import data from Google Drive. First mount it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/DA-LAB/crop_yield.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GSri6jO6bcFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ydata-profiling\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "profile = ProfileReport(df, title=\"Crop Yield Dataset — Profile Report\", explorative=True)\n",
        "profile.to_file(\"profile_report.html\")\n",
        "print(\"Saved: profile_report.html\")"
      ],
      "metadata": {
        "id": "H7ihxXv8bIWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "HGsnHTeUrfEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "before = df.shape[0]\n",
        "df = df.drop_duplicates()\n",
        "after = df.shape[0]\n",
        "print(f\"Removed {before - after} duplicate rows\")"
      ],
      "metadata": {
        "id": "KK8ZpSl2cGC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check info and missing values\n",
        "df.info()"
      ],
      "metadata": {
        "id": "E0Wibl8tkclP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "hulGPIYQdIaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Value Analysis\n",
        "`df.isnull().sum()` shows that all columns have 0 missing values.\n",
        "Therefore no imputation operation is required in this dataset.\n",
        "We proceed directly to outlier handling and encoding steps.\n"
      ],
      "metadata": {
        "id": "vjhziLbyGPaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean of Production:\", df['Production'].mean())\n",
        "print(\"Median of Production:\", df['Production'].median())\n",
        "print(\"Mode of Production:\", df['Production'].mode()[0])"
      ],
      "metadata": {
        "id": "xsd9c3_yu332"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean >> Median >> Mode\n",
        "\n",
        "The mean is much higher than the median, which usually indicates a right-skewed distribution (a few very large values are pulling the mean up).\n",
        "\n",
        "The median (13,804) is much smaller than the mean, meaning that half the data points have production less than about 13,800.\n",
        "\n",
        "The mode is 0, meaning that the most frequent production value is zero — so there are many entries where production was zero or no production recorded."
      ],
      "metadata": {
        "id": "9jS0f6Jr2Z2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution plot of Production\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.histplot(df['Production'], bins=30, kde=True)\n",
        "plt.title(\"Distribution of Production\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gDBM-0D17tXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a copy of the DataFrame to preserve the original\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Apply Label Encoding\n",
        "le = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
        "\n",
        "# Confirm encoding\n",
        "print(\"Encoded DataFrame:\")\n",
        "df_encoded.head()\n"
      ],
      "metadata": {
        "id": "7WNB2q1Oag7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = df_encoded.columns\n",
        "skew_table = df_encoded[cols].skew().sort_values(ascending=False)\n",
        "print(\"Skewness of columns:\\n\", skew_table)"
      ],
      "metadata": {
        "id": "fjDGznYuiEqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skewness Interpretation\n",
        "Skewness values show extremely high right skew for `Pesticide (25.63)`, `Area (21.85)`, `Production (19.29)`, `Fertilizer (13.41)` and `Yield (12.78)`.  \n",
        "This means most observations have very small values while few very large values stretch the tail.\n",
        "\n",
        "Such heavy right skew often occurs in agricultural datasets because only few states/crops have massive scale production while majority are small farmers.\n",
        "\n",
        "Since tree based models are robust to skewness, transformation is optional for them.\n",
        "But PCA + linear models + KNN benefit from normalization + scaling before modeling.\n"
      ],
      "metadata": {
        "id": "tlwVWDWyGnDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute correlation matrix\n",
        "corr_matrix = df_encoded.corr()\n",
        "print(corr_matrix)\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kt6xTuCEcJHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier counts per continuous column (IQR rule)\n",
        "cont_cols = df_encoded.columns\n",
        "outlier_summary = {}\n",
        "for col in cont_cols:\n",
        "    Q1, Q3 = df_encoded[col].quantile(0.25), df_encoded[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lb, ub = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
        "    outlier_summary[col] = int(((df_encoded[col] < lb) | (df_encoded[col] > ub)).sum())\n",
        "print(\"Outlier counts (IQR):\", outlier_summary)"
      ],
      "metadata": {
        "id": "GPwMJqaugpc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(df_encoded.columns):\n",
        "    plt.subplot(2,5, i + 1)\n",
        "    sns.boxplot(y=df_encoded[col])\n",
        "    plt.title(f'Box plot of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UwrDnFpvie8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy to avoid modifying the original DataFrame\n",
        "df_no_outliers = df_encoded.copy()\n",
        "print(f\"Shape before removing outliers: {df_no_outliers.shape}\")\n",
        "# Identify numerical columns again (after encoding)\n",
        "#numerical_cols = df_no_outliers.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Remove outliers for each numerical column using IQR\n",
        "for col in df_no_outliers:\n",
        "    Q1 = df_no_outliers[col].quantile(0.25)\n",
        "    Q3 = df_no_outliers[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df_no_outliers = df_no_outliers[(df_no_outliers[col] >= lower_bound) & (df_no_outliers[col] <= upper_bound)]\n",
        "\n",
        "# Show result\n",
        "print(f\"Shape after removing outliers: {df_no_outliers.shape}\")"
      ],
      "metadata": {
        "id": "sWo9qiKSe1nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(df_no_outliers):\n",
        "    plt.subplot(2,5, i + 1)\n",
        "    sns.boxplot(y=df_no_outliers[col])\n",
        "    plt.title(f'Box plot of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3kANgA9PgjDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalculate correlation matrix without outliers\n",
        "corr_matrix_no_outliers = df_no_outliers.corr()\n",
        "print(\"Correlation Matrix (No Outliers):\")\n",
        "print(corr_matrix_no_outliers)\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix_no_outliers, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap (No Outliers)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "moZj-eqvkJFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features and target\n",
        "X = df_no_outliers.drop('Production', axis=1)\n",
        "y = df_no_outliers['Production']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "AYgFGkAflFwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df_no_outliers.drop('Production', axis=1)\n",
        "y = df_no_outliers['Production']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the models to be evaluated\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
        "    \"XGBoost\": XGBRegressor(random_state=42),\n",
        "    \"AdaBoost\": AdaBoostRegressor(random_state=42),\n",
        "    \"K-Nearest Neighbors\": KNeighborsRegressor()\n",
        "}\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Loop through the models, train, and evaluate\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    results[name] = [r2, rmse, mae]\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(f\"R² Score: {r2:.2f}\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\\n\")"
      ],
      "metadata": {
        "id": "EhBAyHhTME9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Performance Interpretation\n",
        "\n",
        "---\n",
        "\n",
        "### Random Forest\n",
        "- **R² Score:** 0.96  \n",
        "- **RMSE:** 1437.29  \n",
        "The Random Forest model performs very well, explaining 96% of the variance in the target variable. The RMSE value indicates a relatively low average prediction error, showing that this model provides accurate predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### XGBoost\n",
        "- **R² Score:** 0.96  \n",
        "- **RMSE:** 1405.23  \n",
        "XGBoost also achieves an excellent performance, matching the Random Forest in R² and slightly outperforming it in RMSE. This suggests that XGBoost has a slightly better fit and lower prediction error than Random Forest.\n",
        "\n",
        "---\n",
        "\n",
        "### AdaBoost\n",
        "- **R² Score:** 0.72  \n",
        "- **RMSE:** 3658.51  \n",
        "AdaBoost’s performance is noticeably lower than Random Forest and XGBoost. An R² of 0.72 means it explains 72% of the variance, which is decent but less reliable. The higher RMSE indicates larger average prediction errors.\n",
        "\n",
        "---\n",
        "\n",
        "### K-Nearest Neighbors (KNN)\n",
        "- **R² Score:** 0.34  \n",
        "- **RMSE:** 5641.49  \n",
        "KNN shows the weakest performance with only 34% of the variance explained. The high RMSE reflects significant errors in predictions, suggesting KNN is not well-suited for this problem.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary:\n",
        "- **Random Forest** and **XGBoost** are the best-performing models, with very high R² scores and low RMSE values.  \n",
        "- **AdaBoost** is moderately effective but less accurate.  \n",
        "- **K-Nearest Neighbors** performs poorly for this task and likely should not be used in production.\n",
        "\n",
        "Based on these metrics, **XGBoost** might be preferred for slightly better accuracy.\n"
      ],
      "metadata": {
        "id": "-3e61V5UOTWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import (\n",
        "    MinMaxScaler, StandardScaler, RobustScaler, Normalizer,\n",
        "    QuantileTransformer, PowerTransformer\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the scalers\n",
        "scalers = {\n",
        "    \"Min-Max\": MinMaxScaler(),\n",
        "    \"Z-Score (Standard)\": StandardScaler(),\n",
        "    \"Robust Scaling\": RobustScaler(),\n",
        "    \"L2 Normalization\": Normalizer(norm='l2'),\n",
        "    \"Quantile Uniform\": QuantileTransformer(output_distribution='uniform'),\n",
        "    \"Quantile Normal\": QuantileTransformer(output_distribution='normal'),\n",
        "    \"Power Transformer\": PowerTransformer(method='yeo-johnson'),\n",
        "    \"Log Transform\": 'log'\n",
        "}\n",
        "\n",
        "# Store results for scaling comparison\n",
        "scaling_results = {}\n",
        "\n",
        "# Choose the best model\n",
        "best_model = XGBRegressor(random_state=42)\n",
        "\n",
        "# Loop through each scaler\n",
        "for name, scaler in scalers.items():\n",
        "    if name == \"Log Transform\":\n",
        "        # Apply log transform directly\n",
        "        X_train_log = np.log1p(X_train)\n",
        "        X_test_log = np.log1p(X_test)\n",
        "        best_model.fit(X_train_log, y_train)\n",
        "        y_pred = best_model.predict(X_test_log)\n",
        "    else:\n",
        "        # Create a pipeline with the scaler and the model\n",
        "        pipeline = Pipeline([\n",
        "            ('scaler', scaler),\n",
        "            ('model', best_model)\n",
        "        ])\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    scaling_results[name] = [r2, rmse]\n",
        "\n",
        "# Add the baseline (no scaling) result for comparison\n",
        "scaling_results[\"No Scaling (Baseline)\"] = results[\"XGBoost\"][:2]\n",
        "\n",
        "# Display results\n",
        "scaling_results_df = pd.DataFrame(scaling_results, index=['R² Score', 'RMSE']).T.sort_values(by='R² Score', ascending=False)\n",
        "scaling_results_df"
      ],
      "metadata": {
        "id": "SyOH0Td6R7Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Effect of Different Scaling Methods on Model Performance\n",
        "- All scaling methods except **L2 Normalization** result in the **same R² score (0.959)** and **RMSE (~1405.23)**, indicating no significant difference in model performance across these scaling techniques.\n"
      ],
      "metadata": {
        "id": "o1PHQs-1XpNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_full = df_encoded.drop(columns=['Production'])  # target remains 'Production'\n",
        "y_full = df_encoded['Production']\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X_full)  # PCA prefers standardized data\n",
        "pca = PCA(n_components=0.95, svd_solver='full')   # keep 95% variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print(\"PCA components:\", pca.n_components_, \"Explained variance:\", pca.explained_variance_ratio_.sum())"
      ],
      "metadata": {
        "id": "6YSp9pshYRNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Cross-validation on best model\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_rmse = (-cross_val_score(models[\"XGBoost\"], X, y, scoring='neg_root_mean_squared_error', cv=cv)).mean()\n",
        "cv_r2 = (cross_val_score(models[\"XGBoost\"], X, y, scoring='r2', cv=cv)).mean()\n",
        "print(f\"XGB 5-fold CV — RMSE: {cv_rmse:.2f}, R²: {cv_r2:.3f}\")\n"
      ],
      "metadata": {
        "id": "ZPa-z9278pn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsRegressor())\n",
        "])\n",
        "\n",
        "knn_pipeline.fit(X_train, y_train)\n",
        "knn_pred = knn_pipeline.predict(X_test)\n",
        "\n",
        "knn_r2 = r2_score(y_test, knn_pred)\n",
        "knn_rmse = np.sqrt(mean_squared_error(y_test, knn_pred))\n",
        "knn_mae = mean_absolute_error(y_test, knn_pred)\n",
        "\n",
        "print(\"KNN with StandardScaler (Normalized)\")\n",
        "print(\"R2:\", knn_r2)\n",
        "print(\"RMSE:\", knn_rmse)\n",
        "print(\"MAE:\", knn_mae)\n"
      ],
      "metadata": {
        "id": "rk2XrOEK-4rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np\n",
        "\n",
        "models_compare = {}\n",
        "\n",
        "# Separate features and target for the data without outliers\n",
        "X_no_outliers = df_no_outliers.drop('Production', axis=1)\n",
        "y_no_outliers = df_no_outliers['Production']\n",
        "\n",
        "# XGB Original (using data without outliers)\n",
        "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_no_outliers, y_no_outliers, test_size=0.2, random_state=42)\n",
        "xgb_orig = XGBRegressor(random_state=42)\n",
        "xgb_orig.fit(X_train_orig, y_train_orig)\n",
        "pred_orig = xgb_orig.predict(X_test_orig)\n",
        "models_compare[\"XGB Original\"] = [\n",
        "    r2_score(y_test_orig, pred_orig),\n",
        "    np.sqrt(mean_squared_error(y_test_orig, pred_orig)),\n",
        "    mean_absolute_error(y_test_orig, pred_orig)\n",
        "]\n",
        "\n",
        "# PCA on the data without outliers\n",
        "X_scaled_no_outliers = StandardScaler().fit_transform(X_no_outliers)\n",
        "pca = PCA(n_components=0.95, svd_solver='full')\n",
        "X_pca_no_outliers = pca.fit_transform(X_scaled_no_outliers)\n",
        "print(\"PCA components:\", pca.n_components_, \"Explained variance:\", pca.explained_variance_ratio_.sum())\n",
        "\n",
        "# Train-test split for PCA data (from data without outliers)\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca_no_outliers, y_no_outliers, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGB PCA\n",
        "xgb_pca = XGBRegressor(random_state=42)\n",
        "xgb_pca.fit(X_train_pca, y_train_pca)\n",
        "pred_pca = xgb_pca.predict(X_test_pca)\n",
        "models_compare[\"XGB PCA\"] = [\n",
        "    r2_score(y_test_pca, pred_pca),\n",
        "    np.sqrt(mean_squared_error(y_test_pca, pred_pca)),\n",
        "    mean_absolute_error(y_test_pca, pred_pca)\n",
        "]\n",
        "\n",
        "compare_df = pd.DataFrame(models_compare, index=[\"R2\",\"RMSE\",\"MAE\"]).T\n",
        "compare_df"
      ],
      "metadata": {
        "id": "hoZ7ypjNM9Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The XGBoost model trained on the original feature space performed better (R² = 0.959, RMSE ≈ 1405) compared to the XGBoost model trained after PCA dimensionality reduction (R² = 0.934, RMSE ≈ 1775). Although PCA successfully reduced dimensionality while retaining ~99% variance, tree-based models like XGBoost inherently handle nonlinear feature interactions and high dimensional splits effectively, which means they do not significantly benefit from PCA transformation. PCA slightly reduced interpretability and caused information compression which led to reduction in prediction performance."
      ],
      "metadata": {
        "id": "lWjQ_BfjPx4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Conclusion:**\n",
        "\n",
        "In this analysis, multiple regression models were evaluated for agricultural crop production prediction. Among all tested models, XGBoost emerged as the best performing model with very high predictive accuracy (R² = 0.959 and RMSE ≈ 1405). PCA was also applied to reduce dimensionality and retain 95% variance; however, the XGBoost model trained on PCA-transformed features performed comparatively lower (R² = 0.934) than the model trained on the original dataset. This indicates that tree-based ensemble models like XGBoost already learn hierarchical feature splits effectively and do not require dimensionality reduction for performance enhancement. Therefore, the XGBoost model without PCA is selected as the final recommended regression model for crop prediction."
      ],
      "metadata": {
        "id": "sCVys16GP0vL"
      }
    }
  ]
}